{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150b1291",
   "metadata": {},
   "source": [
    "## ðŸ§© Document Chunking Techniques for RAG\n",
    "\n",
    "This notebook implements various **document chunking techniques** to create retrievable passages for a **Retrieval-Augmented Generation (RAG)** pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Fixed Window Chunking\n",
    "Splits text into fixed-size overlapping windows (e.g., every 500 tokens with 100 overlap).\n",
    "\n",
    "### ðŸ”¹ Recursive Character Splitter Chunking\n",
    "Recursively splits text by paragraphs, sentences, or characters to maintain semantic coherence.\n",
    "\n",
    "### ðŸ”¹ Semantic Chunking\n",
    "Uses embeddings or sentence similarity to group semantically related sentences together.\n",
    "\n",
    "### ðŸ”¹ Embedding-Based Chunking\n",
    "Leverages vector similarity to merge or refine chunks based on embedding distances.\n",
    "\n",
    "### ðŸ”¹ Agentic Chunking\n",
    "Dynamically decides how to chunk text based on context using an LLM (e.g., chunk-by-intent or topic).\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ’¡ *Goal:* To evaluate and compare chunking strategies that best balance retrieval precision and generation quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c4747",
   "metadata": {},
   "source": [
    "### Fixed Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16776ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What Are Transformers?\\nTransformers are a type of ', 'neural network architecture that has revolutionize', 'd the field of natural language processing (NLP). ', 'Introduced in a 2017 paper titled \"Attention is Al', 'l You Need\" by Vaswani et al., Transformers are de', 'signed to handle sequential data, such as text, by', ' using a mechanism called self-attention.\\n\\nKey Cha', 'racteristics of Transformers:\\nSelf-Attention Mecha', 'nism: The self-attention mechanism allows Transfor', 'mers to weigh the importance of different words in', ' a sentence when making predictions. This is cruci', 'al for understanding context, especially in long s', 'entences.\\nParallelization: Unlike traditional RNNs', ' (Recurrent Neural Networks), which process data s', 'equentially, Transformers can process multiple wor', 'ds at once, making them faster and more efficient.', '\\nVersatility: Transformers are not limited to lang', 'uage tasks; they can be applied to any problem inv', 'olving sequential data, including tasks like image', ' recognition and time-series forecasting.\\nLLMs vs.', ' Transformers: A Comparative Analysis\\n1. Purpose o', 'f LLMs and Transformer\\nLLMs: Primarily focused on ', 'generating and understanding natural language, LLM', 's are built on various architectures, including Tr', 'ansformers.\\nTransformers: A neural network archite', 'cture used for various tasks, including but not li', 'mited to language modeling.\\n2. Architecture Design', '\\nLLMs: Can be based on different architectures, bu', 't many modern LLMs utilize the Transformer archite', 'cture to achieve state-of-the-art performance.\\nTra', 'nsformers: A specific architecture that uses self-', 'attention and is often employed as the backbone of', ' LLMs.\\n3. Applications\\nLLMs: Used for a wide range', ' of NLP tasks, from text generation and summarizat', 'ion to translation and sentiment analysis.\\nTransfo', 'rmers: Employed not just in NLP but also in other ', 'areas requiring the processing of sequential data,', ' such as speech recognition and computer vision.\\n4', '. Training\\nLLMs: Require massive datasets and comp', 'utational resources for training, often using Tran', 'sformer architecture as a foundation.\\nTransformers', ': The architecture itself, which can be trained on', ' various types of sequential data, is adaptable to', ' different tasks.\\n5. Output\\nLLMs: Generate human-l', 'ike text, making them suitable for applications th', 'at require natural language understanding and gene', 'ration.\\nTransformers: Produce outputs depending on', ' the task at hand, whether it be text, predictions', ', or other types of data sequences.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def fixed_window_splitter(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Splits text at given chunk_size\"\"\"\n",
    "    splits = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        splits.append(text[i:i + chunk_size])\n",
    "    return splits\n",
    "\n",
    "\n",
    "# Read the entire file as a single string\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "splitted_document = fixed_window_splitter(text, chunk_size=50)\n",
    "print(splitted_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b9111",
   "metadata": {},
   "source": [
    "### Fixed Window Chunking with overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a6677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What Are Transformers?\\nTransformers are a type of ', 'rmers are a type of neural network architecture th', 'work architecture that has revolutionized the fiel', 'olutionized the field of natural language processi', 'al language processing (NLP). Introduced in a 2017', 'Introduced in a 2017 paper titled \"Attention is Al', 'led \"Attention is All You Need\" by Vaswani et al.,', '\" by Vaswani et al., Transformers are designed to ', 'ers are designed to handle sequential data, such a', 'uential data, such as text, by using a mechanism c', ' using a mechanism called self-attention.\\n\\nKey Cha', '-attention.\\n\\nKey Characteristics of Transformers:\\n', 'cs of Transformers:\\nSelf-Attention Mechanism: The ', 'tion Mechanism: The self-attention mechanism allow', 'tion mechanism allows Transformers to weigh the im', 'mers to weigh the importance of different words in', 'f different words in a sentence when making predic', 'e when making predictions. This is crucial for und', 's is crucial for understanding context, especially', ' context, especially in long sentences.\\nParalleliz', 'entences.\\nParallelization: Unlike traditional RNNs', 'ike traditional RNNs (Recurrent Neural Networks), ', 't Neural Networks), which process data sequentiall', 'ess data sequentially, Transformers can process mu', 'rmers can process multiple words at once, making t', 'ds at once, making them faster and more efficient.', ' and more efficient.\\nVersatility: Transformers are', 'ty: Transformers are not limited to language tasks', 'ed to language tasks; they can be applied to any p', ' be applied to any problem involving sequential da', 'olving sequential data, including tasks like image', 'ing tasks like image recognition and time-series f', 'on and time-series forecasting.\\nLLMs vs. Transform', '.\\nLLMs vs. Transformers: A Comparative Analysis\\n1.', 'parative Analysis\\n1. Purpose of LLMs and Transform', 'f LLMs and Transformer\\nLLMs: Primarily focused on ', 'rimarily focused on generating and understanding n', ' and understanding natural language, LLMs are buil', 'guage, LLMs are built on various architectures, in', 'us architectures, including Transformers.\\nTransfor', 'ansformers.\\nTransformers: A neural network archite', 'ural network architecture used for various tasks, ', ' for various tasks, including but not limited to l', 'but not limited to language modeling.\\n2. Architect', 'deling.\\n2. Architecture Design\\nLLMs: Can be based ', '\\nLLMs: Can be based on different architectures, bu', 'nt architectures, but many modern LLMs utilize the', 'ern LLMs utilize the Transformer architecture to a', 'er architecture to achieve state-of-the-art perfor', 'te-of-the-art performance.\\nTransformers: A specifi', 'nsformers: A specific architecture that uses self-', 'ture that uses self-attention and is often employe', 'and is often employed as the backbone of LLMs.\\n3. ', 'ackbone of LLMs.\\n3. Applications\\nLLMs: Used for a ', 'ns\\nLLMs: Used for a wide range of NLP tasks, from ', ' of NLP tasks, from text generation and summarizat', 'ation and summarization to translation and sentime', 'nslation and sentiment analysis.\\nTransformers: Emp', 's.\\nTransformers: Employed not just in NLP but also', 'just in NLP but also in other areas requiring the ', 'areas requiring the processing of sequential data,', ' of sequential data, such as speech recognition an', 'peech recognition and computer vision.\\n4. Training', ' vision.\\n4. Training\\nLLMs: Require massive dataset', 'uire massive datasets and computational resources ', 'utational resources for training, often using Tran', 'ng, often using Transformer architecture as a foun', 'chitecture as a foundation.\\nTransformers: The arch', 'ansformers: The architecture itself, which can be ', 'tself, which can be trained on various types of se', ' various types of sequential data, is adaptable to', 'ata, is adaptable to different tasks.\\n5. Output\\nLL', ' tasks.\\n5. Output\\nLLMs: Generate human-like text, ', 'te human-like text, making them suitable for appli', 'm suitable for applications that require natural l', 'at require natural language understanding and gene', 'derstanding and generation.\\nTransformers: Produce ', 'ansformers: Produce outputs depending on the task ', 'pending on the task at hand, whether it be text, p', 'hether it be text, predictions, or other types of ', ', or other types of data sequences.', 'nces.']\n"
     ]
    }
   ],
   "source": [
    "def fixed_window_with_overlap_splitter(text: str, chunk_size: int = 1000, chunk_overlap: int = 10) -> List[str]:\n",
    "    \"\"\"Splits text at given chunk_size, and starts next chunk from start - chunk_overlap position\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start <= len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Read the entire file as a single string\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "splitted_document = fixed_window_with_overlap_splitter(text, chunk_size=50, chunk_overlap=20)\n",
    "print(splitted_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa77d0",
   "metadata": {},
   "source": [
    "### Recursive Character Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec268b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What Are Transformers?', 'Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP). Introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al., Transformers are designed to handle', 'et al., Transformers are designed to handle sequential data, such as text, by using a mechanism called self-attention.', 'Key Characteristics of Transformers:', 'Self-Attention Mechanism: The self-attention mechanism allows Transformers to weigh the importance of different words in a sentence when making predictions. This is crucial for understanding context, especially in long sentences.', 'Parallelization: Unlike traditional RNNs (Recurrent Neural Networks), which process data sequentially, Transformers can process multiple words at once, making them faster and more efficient.', 'Versatility: Transformers are not limited to language tasks; they can be applied to any problem involving sequential data, including tasks like image recognition and time-series forecasting.\\nLLMs vs. Transformers: A Comparative Analysis', 'LLMs vs. Transformers: A Comparative Analysis\\n1. Purpose of LLMs and Transformer\\nLLMs: Primarily focused on generating and understanding natural language, LLMs are built on various architectures, including Transformers.', 'Transformers: A neural network architecture used for various tasks, including but not limited to language modeling.\\n2. Architecture Design', '2. Architecture Design\\nLLMs: Can be based on different architectures, but many modern LLMs utilize the Transformer architecture to achieve state-of-the-art performance.', 'Transformers: A specific architecture that uses self-attention and is often employed as the backbone of LLMs.\\n3. Applications\\nLLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.', 'Transformers: Employed not just in NLP but also in other areas requiring the processing of sequential data, such as speech recognition and computer vision.\\n4. Training', '4. Training\\nLLMs: Require massive datasets and computational resources for training, often using Transformer architecture as a foundation.', 'Transformers: The architecture itself, which can be trained on various types of sequential data, is adaptable to different tasks.\\n5. Output', '5. Output\\nLLMs: Generate human-like text, making them suitable for applications that require natural language understanding and generation.', 'Transformers: Produce outputs depending on the task at hand, whether it be text, predictions, or other types of data sequences.']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "\n",
    "# Read the entire file as a single string\n",
    "with open(\"documents/sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "splitted_document = text_splitter.split_text(text)\n",
    "print(splitted_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eff313",
   "metadata": {},
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aniln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "What Are Transformers? Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP). Introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al., Transformers are designed to handle sequential data, such as text, by using a mechanism called self-attention. Key Characteristics of Transformers:\n",
      "Self-Attention Mechanism: The self-attention mechanism allows Transformers to weigh the importance of different words in a sentence when making predictions.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "This is crucial for understanding context, especially in long sentences.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Parallelization: Unlike traditional RNNs (Recurrent Neural Networks), which process data sequentially, Transformers can process multiple words at once, making them faster and more efficient. Versatility: Transformers are not limited to language tasks; they can be applied to any problem involving sequential data, including tasks like image recognition and time-series forecasting.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "LLMs vs.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Transformers: A Comparative Analysis\n",
      "1.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Purpose of LLMs and Transformer\n",
      "LLMs: Primarily focused on generating and understanding natural language, LLMs are built on various architectures, including Transformers. Transformers: A neural network architecture used for various tasks, including but not limited to language modeling.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "2.\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Architecture Design\n",
      "LLMs: Can be based on different architectures, but many modern LLMs utilize the Transformer architecture to achieve state-of-the-art performance. Transformers: A specific architecture that uses self-attention and is often employed as the backbone of LLMs.\n",
      "\n",
      "--- Chunk 9 ---\n",
      "3.\n",
      "\n",
      "--- Chunk 10 ---\n",
      "Applications\n",
      "LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.\n",
      "\n",
      "--- Chunk 11 ---\n",
      "Transformers: Employed not just in NLP but also in other areas requiring the processing of sequential data, such as speech recognition and computer vision.\n",
      "\n",
      "--- Chunk 12 ---\n",
      "4.\n",
      "\n",
      "--- Chunk 13 ---\n",
      "Training\n",
      "LLMs: Require massive datasets and computational resources for training, often using Transformer architecture as a foundation.\n",
      "\n",
      "--- Chunk 14 ---\n",
      "Transformers: The architecture itself, which can be trained on various types of sequential data, is adaptable to different tasks.\n",
      "\n",
      "--- Chunk 15 ---\n",
      "5.\n",
      "\n",
      "--- Chunk 16 ---\n",
      "Output\n",
      "LLMs: Generate human-like text, making them suitable for applications that require natural language understanding and generation.\n",
      "\n",
      "--- Chunk 17 ---\n",
      "Transformers: Produce outputs depending on the task at hand, whether it be text, predictions, or other types of data sequences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Setup API key\n",
    "# -----------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEY not found. Please set it as an environment variable.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    import re\n",
    "    # Split on period, exclamation, question mark followed by space or line break\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Semantic chunking function\n",
    "# -----------------------------\n",
    "def semantic_chunking_openai(text: str, chunk_size: int = 5, similarity_threshold: float = 0.7) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk text based on semantic similarity using OpenAI embeddings.\n",
    "    Each chunk combines sentences that are semantically close.\n",
    "    chunk_size = number of sentences per chunk\n",
    "    similarity_threshold = minimum cosine similarity to combine sentences\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    # sentences = nltk.sent_tokenize(text)\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    # Get embeddings for each sentence\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=sentences\n",
    "    )\n",
    "    embeddings = [data.embedding for data in response.data]\n",
    "\n",
    "    # Group sentences into semantically similar chunks\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "        if sim >= similarity_threshold and len(current_chunk) < chunk_size:\n",
    "            current_chunk.append(sentences[i])\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Read file and chunk\n",
    "# -----------------------------\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "splitted_document = semantic_chunking_openai(\n",
    "    text=text,\n",
    "    chunk_size=5,              # number of sentences per chunk\n",
    "    similarity_threshold=0.6   # semantic similarity threshold\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Print chunks\n",
    "# -----------------------------\n",
    "for i, chunk in enumerate(splitted_document, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544c903",
   "metadata": {},
   "source": [
    "### Agentic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Setup API key\n",
    "# -----------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEY not found. Please set it as an environment variable.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Agentic chunking function\n",
    "# -----------------------------\n",
    "def agentic_chunking_openai(text_data: str, max_chunk_chars: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a document into semantically coherent chunks using OpenAI LLM.\n",
    "    Each chunk will be â‰¤ max_chunk_chars and preserve meaning.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    I am providing a document below. \n",
    "    Please split the document into chunks that maintain semantic coherence and ensure that each chunk represents a complete and meaningful unit of information. \n",
    "    Each chunk should stand alone, preserving the context and meaning without splitting key ideas across chunks. \n",
    "    Ensure that no chunk exceeds {max_chunk_chars} characters in length, and prioritize keeping related concepts or sections together.\n",
    "\n",
    "    Do not modify the document, just split into chunks and return them as an array of strings, \n",
    "    where each string is one chunk of the document. Return the entire text; do not stop midway.\n",
    "\n",
    "    Document:\n",
    "    {text_data}\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the ChatCompletion API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or gpt-4o if you have access\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract the text\n",
    "    output_text = response.choices[0].message.content\n",
    "\n",
    "    # Try to parse as list if model returned Python-style array\n",
    "    import ast\n",
    "    try:\n",
    "        chunks = ast.literal_eval(output_text)\n",
    "        if isinstance(chunks, list):\n",
    "            return chunks\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Fallback: split by double newlines\n",
    "    chunks = [c.strip() for c in output_text.split(\"\\n\\n\") if c.strip()]\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Usage\n",
    "# -----------------------------\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunks = agentic_chunking_openai(text, max_chunk_chars=1000)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
